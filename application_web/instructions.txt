Cette application Web PHP est une "telecommande" permettant d'interroger une base de donnees a distance, et d'appeler des crawlers depuis l'ordinateur de l'utilisateur pour pouvoir extraire des donnees.
Pour cela, elle utilise une API hebergee sur les serveurs de l'universite de Caen.
Vous pouvez utiliser le serveur web interne de PHP pour utiliser cette application: php -S 127.0.0.1:8000

Les 2 actions possibles sont celles-ci:
Selectionner un crawler -> Choisir une tache (ou plusieurs) -> Effectuer cette tache en local, en permettant de pauser celle-ci si necessaire -> Envoyer les resultats JSON vers une base de donnees.

Ou : Selectionner une source de donnees -> Recuperer les donnees depuis la base de donnees.




Je pense que je m'embrouille dans la logique d'appel de crawler.

On a -> demande un crawler -> recuperation des taches associees -> execution des taches.
Il faudrait enlever l'etape superflue (pour l'instant) de selection des taches. car cela fait refaire un appel a l'API...

Le probleme de communication entre appli PHP et scripts python/js/autres peut peut-etre se regler avec un socket/websocket, utiliser ajax ou autre..
Pour que ces scripts puissent communiquer a l'appli web, faudrait qu'ils soient sur le meme "serveur"....

...Utiliser des mini-frameworks sur chaque crawler 
