[{"text":"charbelle 17h30 lundi pour mettre en place l'API"},{"text":"11h30 vendredi"},{"text":"ou l'api peut prendre cela en compte"},{"text":"efface fichier = stop, presence fichier = go"},{"text":"Etablir communication synchrone entre telecommande et crawler ou systeme et crawler, qui devient en fin de compte asynchrone"},{"text":"solution connection crawler : pause/stop\nentre chaque etape, il verifie qu'il a le droit d'enchaine (genre le contenu d'un fichier ou sa presence ou un cookie)"},{"text":"hmmm ok."},{"text":"et du coup, au lancement des scripts, on marque status:not_done par defaut."},{"text":"pour savoir si tout est fini.. une requete vers gestionnaire. qui fait une troisieme fonction:\nil regarde si dans tout les fichiers de sortie json, que y'a un parametre status:done."},{"text":"du coup."},{"text":"oh et gestionnaire peut avoir une autre fonction qui pause si on envoie $_POST[pid]"},{"text":"gestionnaire... appelle X commandes, et recup leur PID, et demande a mettre resultat dans cache/machin.json en fin d'execution. gestionnaire est donc redisponible.\nla requete vers gestionnaire renvoie le pid?\ndu coup on peut avoir des boutons pause. Normalement.\nDu coup maintenant.. comment savoir que tout les scripts sont termines?"},{"text":"child process?"},{"text":"mais si bouton pause/stop de tout... je sais pas."},{"text":"interface : mettre bouton pause/stop.\nsi mettre bouton par tache.. faut recuperer le uid depuis l'interface.. ce qui semble etre complique."},{"text":"https://stackoverflow.com/questions/1470910/invoke-external-shell-script-from-php-and-get-its-process-id\nhttps://unix.stackexchange.com/questions/2107/how-to-suspend-and-resume-processes"},{"text":"Vendredi 14h la reunion?"},{"text":"BON BAH ESSAYONS"},{"text":"ET PAS BESOIN DE WEBSOCKET/SSE.... NORMALEMENT."},{"text":"pour chaque tache, on fait un div \"tache X a recupere X donnees\"."},{"text":"a chaque script on associe un fichier \"currentLog\".txt en milieu d'execution"},{"text":"ok donc.....\n\ntelecommande => lance scripts en arriere-plan\n\n+recupere progression via poll ajax.\nhmmmmmmmmmmm................\nHM...........................\nCA PEUT PEUT-ETRE SE FAIRE"},{"text":"But how do you get progress information? Simple, just get your long running script to report its progress to a file or a database, and use another, web-based script to read the progress / show the final result."},{"text":"http://symcbean.blogspot.com/2010/02/php-and-long-running-processes.html"},{"text":"https://blog.samuel.ninja/the-tutorial-for-php-websockets-that-i-wish-had-existed/"},{"text":"puis apres... on verra"},{"text":"et de communiquer avec"},{"text":"essayons deja de faire un host de websocket"},{"text":"https://www.twilio.com/blog/create-php-websocket-server-build-real-time-even-driven-application"},{"text":"faut juste penser a faire une methode pour host/arreter de host correctement"},{"text":"RATCHET EST UNE SOLUTION 100% PHP, CE QUI PEUT ETRE PRATIQUE VU QU'ON SOUHAITE DISTRIBUER NOTRE TRUC COMME CA."},{"text":"DONC COMMENT FONCTIONNE UNE WEBSOCKET"},{"text":"cette websocket recupere la liste des taches et execute donc les scripts, tout en envoyant des messages de temps en temps."},{"text":"Pour faire cela, il etablit donc une connection avec la websocket (js ou php on verra), et met des trucs"},{"text":"La solution la plus approprie semble etre de faire un service websocket\nL'application web recoit une liste de taches de... lui-meme... et souhaite appeler X scripts (et recuperer les donnees) TOUT EN AFFICHANT LA PROGRESSION (juste debut/fin pour l'instant)"},{"text":"et qui informe donc \"l'application\""},{"text":"je veux faire un \"manager\" qui regarde si cet appel de script est fini ou non"},{"text":"je peux faire un appel de script depuis PHP, certes"},{"text":"A chaque fois je reviens sur le meme probleme...."},{"text":"essayons tout de meme"},{"text":"https://datto.engineering/post/powering-a-live-ui-with-server-sent-events"},{"text":"sse ne supporte pas l'envoi... et je suis pas sur que le protocole websocket soit la solution"},{"text":"MON CAS C'est :\n\nDES LE LANCEMENT DE LA PAGE, JE VEUX EXECUTER X SCRIPTS CRAWLERS\n\nQUAND CEUX-CI FINISSENT, JE VEUX FAIRE QUELQUE CHOSE.. GENRE AFFICHER UN MESSAGE DE SUCCES OU REDIRIGER LA PAGE"},{"text":"HMM QUOI FAIRE ALORS"},{"text":"DONC ON NE VEUT PAS."},{"text":"CA CAUSE DES PROBLEMES DE SECURITE"},{"text":"AJAX ET LES FICHIERS LOCAUX..."},{"text":"OK PROBLEME"},{"text":"libraries async php"},{"text":"https://amphp.org/"},{"text":"https://reactphp.org"},{"text":"comme parametre recuperable de ajax, on fera juste : AllDone = False, ....TaskArrayStatus..? = ??\ndata = objet json."},{"text":"selection taches -> AFFICHAGE PAGE QUI ENVOIE DES TRUCS AJAX TOUTES LES X SECONDES (10) -> AFFICHAGE DE TRUCS SUR LA PAGE SELON PARAMETRES DE CE MANAGER, QUAND TOUT FINI ALORS REDIRECTION ET MESSAGE \"VOUS VOULEZ VRAIMENT ENVOYER X DONNEES?\""},{"text":"REPRENONS"},{"text":"donc en fin de compte faut bien faire un manager dans callcrawlers. bon."},{"text":"apres on travail sur plusieurs taches, mais on peut juste  faire un parametre alltasksdone = true ou false a la toute fin du script php, pas de soucis."},{"text":"AH MAIS ATTENDS, ON PEUT JUSTE FAIRE \nSUCCEES: IF TASKS STATUS = TRUE THEN REDIRECT... JE SUPPOSE? ON VERRA."},{"text":"donc dans php, une redirection asynchrone..? pourquoi pas"},{"text":"hmmm"},{"text":"bon bref, appels ajax/jquery sur la page manager, qui nous dit des trucs, et quand ca renvoie status:DONE ou quelque chose du genre, la page utilisateur se transforme en page \"VOULEZ VOUS INSERER X DONNEES DANS LA BDD???\" en passant des parametres jsonarray et size"},{"text":"Ce que je peux faire c'est :\nSelection taches/crawlers ->\nAffichage d'une page utilisateur -> Questionnement d'une autre page \"manager\" ->\nle questionnement renvoie un truc du genre $progression1 = '2345 loops' puis '4534 loops' etc..."},{"text":"Pour afficher a l'utilisateur la progression des scripts/crawlers sur la page PHP, il faudra forcement utiliser du JS, ou un websocket, mais bon..."},{"text":"https://github.com/PostgREST/postgrest utile a savoir que cela existe pour un vrai cas professionnel, mais bon on va ecrire notre propre API"},{"text":"mardi 15h reunion"},{"text":"Extra:\n- Faire des diagrammes d'utilisation/relation/condition(?)\n- Convertir mon script pour discord en un truc fait sur python, si en utilise scrapy pour faire du managing/middleware\n- Trouver d'autres sources :hap:\n- Permettre d'extraire des dms sur discord"},{"text":"1. Télécommande des crawlers:\n-Choisir un crawler via une appli web en local\n-S'authentifier auprés de l'API\n-Demander une liste des tâches correspondant au crawler (subreddit X, chaîne X, etc...)\n-Faire la/les tâches (en prenant en compte la progression de celle-ci, en sauvegardant quelque part dans la BDD)\n-Envoyer les données, et peut-etre faire une copie local en cache juste au cas où\n\n2. Api gérant l'accés à la base de données:\n- Enregistrer des crawlers dans une table séparée\n- Faire une table par liste de tâches?\n- Faire un service d'authentification qui donne des Tokens\n- Mettre dans une BDD quand on reçoit des données\n- Permettre d'extraire des données depuis la BDD"},{"text":"Envoyer un message à Mr Ledesert ainsi\n\"\"\"\nBonjour,\nNotre groupe vous contacte pour notre projet, de la part de Mr Rioult,\nNous souhaitons mettre en place une API sur le serveur du département en utilisant un framework PHP (Laravel, Symphony, Lumen ou autre) pour permettre de s'identifier à distance, et de pouvoir réaliser certains actions lorsqu'on est identifié.\nÉtant donné que vous avez mis en place une API documentée ici : https://documenter.getpostman.com/view/570147/TVRrU4GY dans le cadre du cours de Développement d'applications mobiles, serait-il possible d'avoir quelques pistes pour la mise en place d'une API, et la documentation de celle-ci?\nCordialement.\n\"\"\""},{"text":"https://www.postman.com/api-documentation-tool/"},{"text":"tuto laravel API : https://laravel.sillo.org/une-api-avec-laravel-6/ semble être approprié"},{"text":"Dans la BDD on pourrait avoir X tables:\n1 table qui contient les données récupérées\n1 table qui contient les crawlers\n1 table PAR crawler qui indiquent la liste de tâches (et d'autres infos genre nb de trucs récupérées, dernier lancement, etc..)"},{"text":"3. Développement de nouveaux crawlers\n- Refaire le crawler discord, car js est pas compatible avec scrapy...\n- Trouver de nouvelles sources\n- Magie"},{"text":"2. Api gérant l'accés à la base de données:\n- Enregistrer des crawlers dans une table séparée\n- Faire un service d'authentification et de rendre des Tokens\n- Mettre dans une BDD quand on reçoit des données"},{"text":"1. Télécommande des crawlers:\n-Choisir un crawler via une appli web en local\n-S'authentifier auprés de l'API\n-Demander une liste des tâches (subreddit X, chaîne X, etc...)\n-Faire la/les tâches (en prenant en compte la progression de celle-ci, en utilisant le combo Path/Path_Index ou un truc du genre\n-Envoyer les données, et peut-etre faire une copie local en cache juste au cas où"},{"text":"Bon : On peut répartir le travail ainsi:\n1. Développement de la télécommande des crawlers/crawler incrémental via un script PHP\n2. Développement de l'API à mettre sur le site du département, et du côté administrateur\n3.Développement de nouveaux crawlers"},{"text":""},{"text":""},{"text":""},{"text":"commence a devenir compliqué l'architecture"},{"text":"crawler parle a api, api lui dit si il reste des trucs"},{"text":"Separer projet en 3:\n1. la telecommande de script en locale\n2. l'API sur UNI pour inserer/recup donnees\n3.ameliorer les crawlers : utiliser scrappy pour tout gerer. tant pis le script json"},{"text":"https://laravel.com/docs/5.8/api-authentication API TOKEN"},{"text":"differencier script de crawl en local, et envoi api a distance sur uni"},{"text":"prendre contact ledesert : API laravel, comment il a fait; des conseils svp"},{"text":"https://stackoverflow.com/questions/4064444/returning-json-from-a-php-script?rq=1 facon simple (en meme temps c'est ce qu'il faut faire) pour que le script php renvoie uniquement le json. bon on peut aussi faire un dump en plus du seul echo"},{"text":"just faire attention avec exec() dans php pour ne pas avoir de code arbitraire.."},{"text":"ouais ca me parait plus simple et moins prompt aux bugs"},{"text":"Ou juste FORM PHP => Script => PHP => Envoi dans BDD"},{"text":"Exemple d'execution\nPage PHP => selection de truc à faire =>\nrecup les trucs => execution d'un machin overseer avec 3 arguments (js ou PHP ou autre..) => utilisation du crawler => retour données vers PHP => POST du JSON.\nmettre des trucs => interrogation de la BDD. faire un DUMP. puis.... voilà"},{"text":"Pour résumer avant de retravailler demain :\n1. Faire un plan pour unifier les scrappers/crawlers (j'ai mon idée de PHP -> overseer -> crawlerX -> overseer -> PHP)\nFormat données final : TEXT / SOURCE / INDEX_PATH / INDEX_ID / REAL_ID\n2. mettre en place la BDD de test sur le pc portable\n3. Construire le overseer... Permettre appel de crawlers à partir de 3 arguments : Source, Limite, et Path (pour des trucs compliqués ou bizarre)\net puis euhhh... faire un truc api php ou un fichier js.. utiliser un framework node?\n4. Mettre en place la page Web pour appeler le Overseer/interroger la BDD et recevoir un dump\n5. Faire le rapport."},{"text":"rahhh va falloir que je travaille sur mon point faible"},{"text":"ou utiliser AJAX"},{"text":"ou en PHP?"},{"text":"vu qu'on fait du PHP peut-etre juste faire le overseer en js.."},{"text":"python peut faire def func():\ndo stuff\n\net if \\_\\_name\\_\\_ == '\\_\\_main\\_\\_':\nexecution de truc en script donc juste\nfunc()"},{"text":"recevoir donnée avec stdin pipe classique"},{"text":"https://www.php.net/manual/en/function.proc-open.php pour faire une commande/processus asynchrone"},{"text":"Je commence à m'embrouiller les pinceaux, arrêtons ici."},{"text":"Notre page web (modifiable plus tard si nécessaire) a deux utilisations possibles :\n\nCas 1) Un appel du Overseer (ou de lui-meme du coup) pour recuperer des données JSON, et mettre celles-ci dans la BDD\nOn a donc trois arguments possibles : 1. La source  2. Une limite de crawl (100 pages, 100 posts, 100 commentaires...) 3. Une concaténation de \"sous-arguments\" de taille inconnue pour un truc spécifique (Genre /Path1/Path2/Path3...) (Peut-être ne pas prendre en compte pour l'instant car je ne connais aucun cas utilisant cela actuellement).\n\nEt puis le Overseer/lui-meme se debrouille pour appeler le crawler correspondant, et de renvoyer le JSON équivalent.\nJe me demande si on pourrait faire une mini API interne du site pour simplifier ces appels, donc \"sans\" Overseer..\n\nCas 2) Appeler la BDD (GET) pour récupérer des données, faire un gros dump wala."},{"text":"https://kevinsmith.io/modern-php-without-a-framework"},{"text":"et le overseer, on bidouillera"},{"text":"https://stackoverflow.com/questions/14047979/executing-python-script-in-php-and-exchanging-data-between-the-two bon suivre cela"},{"text":"donc en fait.. mon overseer faudrait le faire en PHP directement? ok..."},{"text":"bon."},{"text":"If you are experiencing the NULL return from the python script, it may mean that escapeshellarg() is taking the double quotes away from the encoded JSON, rendering it invalid JSON, therefore causing json.loads() to fail. Try using base64_encode(); instead of escapeshellarg(); in php and import base64/base64.b64decode() in python. Also, don't forget to wrap your path in double quotes if it contains spaces 'python \"path/that/has spaces/script.py\" ' as was my case. Cheers! :)"},{"text":"js peut faire... on verra"},{"text":"python peut faire print json.dumps(truc)"},{"text":"dans le fichier qui envoie les données, on fait un print ou autre pour utiliser stdin classique."},{"text":"on envoie les arguments en json car c'est un langage COMMUN de communication, pratique"},{"text":"Donc on appelle le script via PHP, on fait un json_decode si besoin (dans notre cas.. non?) et dans le script voulu, on fait un try avec sys.argv ou autre selon le langage"},{"text":"ah voilà exactement ce que je me demandais."},{"text":"https://stackoverflow.com/questions/14047979/executing-python-script-in-php-and-exchanging-data-between-the-two"},{"text":"et donner les privileges adéquats aux fichiers (car on est dans un truc apache PHP)"},{"text":"et on utilise des shebang pour que ça fonction"},{"text":"apparament cette fonction permet d'executer une commande (donc appeler un script) via shell, et de recup le resultat en string. Cela semble etre de la magie noire mais bon."},{"text":"https://www.php.net/manual/en/function.shell-exec.php"},{"text":"Bon, en gros, Depuis PHP, un appel d'un AUTRE script (le overseer qui prends 2 args)\nqui va décider d'appeler un crawler pré-défini\nce crawler renvoie des données JSON (si c'est js => js ok, mais si c'est un autre... comment appeler 1 script d'un autre langage...)\nque le overseer renvoie à la page\net la page met dans le BDD (ou stocke en local dans le cadre de nos tests vu que c'est du JSON)."},{"text":"ce qui fait beaucoup de liens bizarre."},{"text":"le truc final c'est... peut-on juste faire ce overseer en python ou faut-il un programme spécialisé pour récupérer/convertir les données des autres scripts?\ncar on peut juste faire un stockage intermédiaire en json en local. pourquoi pas.\nmais il serait plus simple de faire page.php => overseer => script...reddit => DONNEES DE CE SCRIPT RETOUR VERS OVERSEER => RENVOIE VERS PAGE.PHP => utilisation de POST en php pour envoyer vers la BDD."},{"text":"ça commence déja à prendre forme"},{"text":"On appelle le \"overseer\"  depuis la page PHP (ou en ligne de commande) avec 3 arguments:\nsource de type texte indique quel script/service sera utilisé (if source=reddit then CALL scripts/crawlerReddit/machin.py(args.split))\nargs indique des arguments concatenés à utiliser depuis les autres scripts, genre... France/art/100 pour indiquer que l'on fait le crawl sur le subreddit France, en utilisant le mot-clé \"Art\", et en faisant MAX 100 posts.\nreturn_type = return ou save pour faire un return ou un stockage local à la fin du script.\n\nAutre cas: Overseer Discord NOMDUBOT/FETCH(OU DM PLUS TARD SI J'AI LE TEMPS) save\n\nCas spécial à implémenter en plus : All (où il fait tout, en donnant des limites arbitraires genre 10)\n\neh puis à la fin de ce truc overseer, SOIT on renvoie les données avec un return ou un truc du genre, soit on stock localement les données dans le répertoire où se situe le overseer. (ou un dossier à côté)"},{"text":"déja mettons en place la logique d'appel des crawlers."},{"text":"bon."},{"text":"depuis la page web on aura genre une fonction d'appel du \"overseer\" onclickSEND(arguments=arguments)\n#source=reddit, subreddit=france, keywords = [art,artiste,artistement]\net donc le overseer va faire des trucs selon ces valeurs d'arguments. (langage utilisée... qui sait, php?)\nenfin, avec les données brut/fichier json assemblé, il va mettre ça dans la BDD avec un POST PHP.\n#finally BDD.send(result) etc je sais plus la syntaxe\nou alors il le fait X fois par type de crawler appelé... ce qui serait... peut-etre l'idée."},{"text":"ah en plus faut faire un appel du script via PHP au final"},{"text":"Probleme principal est le numéro 3, si on reste sur du python simple, il existe un probléme, certains crawlers utilisent d'autres langages (appel asynchrone en js pour le crawler discord par exemple.)\nfaire simplement un \n# script = machinmachin\n#script.execute(argument=machin1)\npeut etre delicat.."},{"text":"Tâches prévus groissiérement :\n1. Unifier les résultats provenant des scrappers/crawlers\n2. Mettre en place une BDD \"test\" sur le pc portable\n3. Faire un truc \"crawler overseer\" qui permet d'appeler les crawlers avec des arguments genre \"call reddit crawler with subreddit=france, post_limit=10,keywords =art,artiste,artistement\" etc, et mettre en place le systéme sub_id pour incrémentation (ne pas avoir à faire un crawling sur toute le site/channel...)\n4. faire une page Web qui appele le crawler overseer, ou recevoir/envoyer des données en utilisant PHP pour la BDD\n5. Faire le rapport"},{"text":"Notes express :\nPour unifier les résultats des scrappers, il me semble qu'une composition possible soit:\ntext=text ;\nsource = reddit/discord/forum.fr/etc... ;\nsub_path = 0 si y'en a pas ou si c'est un cas particulier, sinon on peut faire genre = post_id (cas reddit) ou = channel_id (cas discord)  séparation de sous-chemin avec / en textuel..? on verra lors de l'implémentation;\nsub_id = valeur incrémentée automatiquement selon source sub pour avoir une indexation des sous-parties dans la bdd ;\nreal_id = valeur id de référence si il y en a une. on pourra faire depuis le crawler incrémental des trucs avec, par exemple : #when LAST.bdd.real_id = result.real_id then FORCE stop scrapping ;"},{"text":"amélioration de ce scrapper : permettre le scrapping de messages privés... peut-être sans utiliser un bot, mais en utilisant le token d'authentification d'un utilisateur"},{"text":"rendez-vous tout les vendredi à 10h"},{"text":"Bon déja faisons un trello/truc sur la forge pour s'organiser"},{"text":"liste de course : cocher dans la liste pour dire \"j'ai crawlé cette partie là\", donc sur n'importe quelle machine : savoir où s'est arrete le crawl general pour continuer\n\ncrawler start et crawler stop..?"},{"text":"Unifier les résultats des crawlers : par exemple des informations en plus genre :SOURCE:reddit ou SOURCE:discord"},{"text":"genre post id : 2004 alors faire au dessus de 2005"},{"text":"genre le crawler web : quand on relance le crawler : regarder le dernier point crawlé, et décider de la suite des données, si il y en a"},{"text":"- stabiliser les crawler - paramétrage dans une base de données - relancer le crawler incrémental"},{"text":"(j'espere ça serait pratique)"},{"text":"- PostGres : stockage possible en JSON"},{"text":"Choix de technologies :\n- hébergement sur les serveurs du département -> Apache / PHP\n- BD : mySQL ou PostGres"},{"text":"Application web (API simple PHP, host => serveur departement donc tech restreinte, apache php forcé) :\n- recevoir du texte en provenance des crawler -> stockage en BD : accepter un POST de data qui est enregistré en BD\n- stockage en BD -> générer des dumps au format adéquat pour Carlos : générer un accès GET****"},{"text":"celle-ci contient du texte en provenance des crawlers/scrappers"},{"text":"bon ok : on fait un bdd accessible via web"},{"text":"bon bah pas de chatbot nous-meme :("},{"text":"application web pour recuperer les resultats dans la base de donnees"},{"text":""},{"text":""},{"text":"https://www.kaggle.com/breandan/french-reddit-discussion FR REDDIT"},{"text":"https://lionbridge.ai/datasets/20-best-french-language-datasets-for-machine-learning/? FR SETS"},{"text":"https://www.zapptales.com/en/download-facebook-messenger-chat-history-how-to/"},{"text":"https://chatbotslife.com/replicate-your-friend-with-transformer-bc5efe3a1596?gi=1f06088aa0d6"},{"text":"Bonjour, \n\nsuite à la réunion de la semaine dernière je reprends contact avec vous.\nCarlos a publié le code de Marion ici : https://github.com/carlitoselmago/marionbotapi\nFrançois, si vous connaissez des personnes travaillant avec les transformers, n'hésitez pas à partager avec eux cette page et à nous mettre en contact.\nNous sommes toujours intéressés par la constitution d'un corpus conversationnel francophone - idéalement sur l'art.\nEt merci d'avance pour votre investissement sur ce projet.\nBonne journée,"},{"text":"Du coup si je récapitule on doit  :\n- Trouver des scripts de films pour en exporter répliques\n-Trouver des forums et  des serveurs et demander l'autorisation des propriétaires\n-Voir si c'est en accord avec la rgpd \n- Trouver et prendre en main les outils d'extraction"},{"text":"https://towardsdatascience.com/web-scraping-of-10-online-shops-in-30-minutes-with-python-and-scrapy-a7f66e42446d"},{"text":"Sinon : MechanicalSoup ou Scrapy"},{"text":"Si javascript : selenium"},{"text":"Si API: utiliser l'API"},{"text":"bon ok en gros:"},{"text":"Si le truc propose un API genre reste faut utiliser l'API lol"},{"text":"sinon go MS"},{"text":"Si javascript alors selenium"},{"text":"bon en gros selenium est un navigateur a 100%, la ou MS interagis avec des elements HTML."},{"text":"et il peut aussi faire de l'interaction web, cool"},{"text":"mechanicalsoup c'est un truc qui combine requests et beautifulsoup"},{"text":"faudra voir quelles sont nos sources"},{"text":"ouais selenium est bien je pense."},{"text":"On peut aussi vraiment customiser scrapy avec des proxys, pipeline, etc.\nsi notre truc est simple on peut juste faire un machin selenium."},{"text":"on peut config scrapy pour regarder JUSTE le fichier html, alors que selenium s'en va visiter tout les fichiers js,css, img"},{"text":"Comparaison:  scrapy fait just du DL html, parse les donnees, et les sauvegardent.\nSelenium automatise de l'interaction web (genre remplir un formulaire de recherche puis cliquer sur \"next page\")"},{"text":"selenium est une bonne idee pour les trucs o'u il faut simuler la requete avec des headers/cookies/payload, c'est un \"headless browser\" pis on scrap/parse ce qu'il recupere."},{"text":"https://ledatascientist.com/introduction-au-web-scraping-avec-python/\nhttps://ledatascientist.com/web-scraping-python-avec-selenium/"},{"text":"https://github.com/Tyrrrz/DiscordChatExporter nice un gui mais ca casse les ToS OOPS"},{"text":"https://dht.chylex.com/ extract discord chat history via navigator"},{"text":"tri!fddffkldkjs"},{"text":"Ton avatar: <https://cdn.discordapp.com/avatars/82963435289055232/f9a65eff2e2ad659d63c5deb7127979a.png>"},{"text":"tri!pfp"},{"text":"tri!pfp"},{"text":"Ton avatar: <https://cdn.discordapp.com/avatars/82963435289055232/f9a65eff2e2ad659d63c5deb7127979a.png>"},{"text":"tri!avatar"},{"text":"Pong."},{"text":"tri!ping"},{"text":"<@82963435289055232>, please wait 1.1 more second(s) before reusing the `ping` command."},{"text":"tri!ping"},{"text":"<@82963435289055232>, please wait 3.3 more second(s) before reusing the `ping` command."},{"text":"tri!ping"},{"text":"Pong."},{"text":"tri!ping"},{"text":"tri!role un deux"},{"text":"Il faut mettre des arguments, <@82963435289055232>!\nUtilisation de la commande: `tri!role <user> <role>`"},{"text":"tri!role"},{"text":"<@82963435289055232>, Cela n'est pas un chiffre valide."},{"text":"tri!prune"},{"text":"<@82963435289055232>, Cela n'est pas un chiffre valide."},{"text":"tri!prune"},{"text":"prune"},{"text":"Ton avatar: <https://cdn.discordapp.com/avatars/82963435289055232/f9a65eff2e2ad659d63c5deb7127979a.png>"},{"text":"tri!avatar"},{"text":"Ton avatar: <https://cdn.discordapp.com/avatars/82963435289055232/f9a65eff2e2ad659d63c5deb7127979a.png>"},{"text":"tri!avatar"},{"text":"t!prune 2"},{"text":"t!avatar"},{"text":"MERCI."},{"text":"tri!prune 1"},{"text":"Ton avatar: <https://cdn.discordapp.com/avatars/82963435289055232/f9a65eff2e2ad659d63c5deb7127979a.png>"},{"text":"tri!avatar"},{"text":"Limitation API: certains messages ont ete envoyés il y a plus de 2 semaines."},{"text":"tri!prune 4"},{"text":"Limitation API: certains messages ont ete envoyés il y a plus de 2 semaines."},{"text":"tri!prune 1"},{"text":"l'avatar de ElScrappito : <https://cdn.discordapp.com/embed/avatars/4.png>\nl'avatar de twis : <https://cdn.discordapp.com/avatars/82963435289055232/f9a65eff2e2ad659d63c5deb7127979a.png>"},{"text":"tri!avatar <@!826799353343377449>  <@!82963435289055232>"},{"text":"l'avatar de ElScrappito : <<https://cdn.discordapp.com/embed/avatars/4.png>\nl'avatar de twis : <<https://cdn.discordapp.com/avatars/82963435289055232/f9a65eff2e2ad659d63c5deb7127979a.png>"},{"text":"tri!avatar <@!826799353343377449>  <@!82963435289055232>"},{"text":"Ton avatar: <https://cdn.discordapp.com/avatars/82963435289055232/f9a65eff2e2ad659d63c5deb7127979a.png>"},{"text":"tri!avatar"},{"text":"Ton avatar: <https://cdn.discordapp.com/avatars/82963435289055232/f9a65eff2e2ad659d63c5deb7127979a.jpg>"},{"text":"tri!avatar stp"},{"text":"tu veux kick:  ElScrappito"},{"text":"tri!fakekick <@!826799353343377449>"},{"text":"<@82963435289055232>, Il faut tag quelqu'un pour pouvoir le kick."},{"text":"tri!fakekick oops"},{"text":"Nom de commande?  : args-info\nArguments? : je,ne,suis,pas,une,vrai,commande"},{"text":"tri!args-info je ne suis pas une vrai commande"},{"text":"TOI ET MOI ElScrappito, toi et moi.."},{"text":"Server name: pastebin\nTotal members: 2"},{"text":"tri!server"},{"text":"tri!server info please"},{"text":"Boop."},{"text":"tri!beep test2"},{"text":"Pong."},{"text":"tri!ping test"},{"text":"Pong."},{"text":"tri!ping"},{"text":"Pong."},{"text":"${prefix}ping"},{"text":"${prefix}ping"},{"text":"tri!ping"},{"text":"Pong."},{"text":"tri!ping"},{"text":"Pong."},{"text":"ping"},{"text":"tri!ping"},{"text":"Pong."},{"text":"!ping"},{"text":"!ping"},{"text":"!ping"},{"text":"twitch.tv/holofightz (tous les samedis vers 17-20h)"},{"text":""},{"text":"https://heartbeat.fritz.ai/building-a-conversational-chatbot-with-nltk-and-tensorflow-part-1-f452ce1756e5 Autre article détaillant la mise en place d'un chatbot (avec un corpus particulier, le tennis)"},{"text":"ok on a fait un fichier excel, mettre le backlog et les infos sur une autre page."},{"text":"Plan ->\n1. Faire un modele de données meilleur que \"text:text\"\n2. Le mettre dans une BDD et organiser tout ça\n3. Faire quelques projets tensorflow/scikit-learn pour prendre la main en machine learning\nPuis faire le chatbot:\n4. Constituer des jeux de données différents, un jeu de données \"entraînement\" et un jeu de données \"test\", soit un truc:\ninput = \"blablablabla\" vaut index 564645 , réponse est \"bliblibliblu\", la réponse voulue est bien \"blibliblibli\" =  quasi-bonne réponse, le modéle reçoit des points.\n5."},{"text":"https://openclassrooms.com/forum/sujet/creer-une-chatbot-avec-python jsp"},{"text":"https://www.tophebergeur.com/blog/projet-chatbot-python/ jsp"},{"text":"scikit-learn comme base de modéles?"},{"text":"https://www.youtube.com/watch?v=QZXIBVpVYss détails lors de la formation (aprés entrainement/test) du chatbot"},{"text":"https://blog.tensorflow.org/2019/05/transformer-chatbot-tutorial-with-tensorflow-2.html Chaque etape de la mise en place d'un chatbot est détaillée, utile si on se tape un blanc"},{"text":"--------------------------------------------------------------------------------------------------------------------"},{"text":"https://www.kaggle.com/breandan/french-reddit-discussion FR REDDIT"},{"text":"https://lionbridge.ai/datasets/20-best-french-language-datasets-for-machine-learning/ FR"},{"text":"https://github.com/alexa/Topical-Chat"},{"text":"https://ccdb.cs.cf.ac.uk/ faut s'inscrire"},{"text":"https://github.com/PolyAI-LDN/conversational-datasets"},{"text":"https://breakend.github.io/DialogDatasets/"},{"text":"http://freeconnection.blogspot.com/2016/04/conversational-datasets-for-train.html"},{"text":"http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html il utilise deja"},{"text":"é è à ù â ê î ô û"},{"text":"j'aime l'art."},{"text":"JE SUIS D'ACCORD MEME SI JE SUIS LA MEME PERSONNE QUI PARLE"},{"text":"L'ART C'EST TROP BIEN"},{"text":"assez de messages"},{"text":"devrais y avoir"},{"text":"il"},{"text":"ok"},{"text":"100"},{"text":"99"},{"text":"98"},{"text":"97"},{"text":"96"},{"text":"95"},{"text":"94"},{"text":"93"},{"text":"92"},{"text":"91"},{"text":"90"},{"text":"89"},{"text":"88"},{"text":"87"},{"text":"86"},{"text":"85"},{"text":"84"},{"text":"83"},{"text":"82"},{"text":"81"},{"text":"80"},{"text":"79"},{"text":"78"},{"text":"77"},{"text":"76"},{"text":"75"},{"text":"74"},{"text":"73"},{"text":"72"},{"text":"71"},{"text":"70"},{"text":"69"},{"text":"68"},{"text":"67"},{"text":"66"},{"text":"65"},{"text":"64"},{"text":"63"},{"text":"62"},{"text":"61"},{"text":"60"},{"text":"59"},{"text":"58"},{"text":"57"},{"text":"56"},{"text":"55"},{"text":"54"},{"text":"53"},{"text":"52"},{"text":"51"},{"text":"50"},{"text":"49"},{"text":"48"},{"text":"47"},{"text":"46"},{"text":"45"},{"text":"44"},{"text":"43"},{"text":"42"},{"text":"41"},{"text":"40"},{"text":"39"},{"text":"38"},{"text":"37"},{"text":"36"},{"text":"35"},{"text":"34"},{"text":"33"},{"text":"32"},{"text":"31"},{"text":"30"},{"text":"29"},{"text":"28"},{"text":"27"},{"text":"26"},{"text":"25"},{"text":"24"},{"text":"23"},{"text":"22"},{"text":"21"},{"text":"20"},{"text":"19"},{"text":"18"},{"text":"17"},{"text":"16"},{"text":"15"},{"text":"14"},{"text":"13"},{"text":"12"},{"text":"11"},{"text":"10"},{"text":"9"},{"text":"8"},{"text":"7"},{"text":"6"},{"text":"5"},{"text":"4"},{"text":"3"},{"text":"2"},{"text":"1"}]